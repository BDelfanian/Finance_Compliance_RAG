"""
STEP 5 â€” Citation-Bound Answer Generation
Uses STEP 4 retrieval output to generate multi-regulator, citation-bound answers
with GPT-5 mini.
"""

import os
import json
import hashlib
from datetime import datetime
from run_embeddings_retrieval import retrieve, vector_store, embed_text
import openai


CACHE_DIR = "data/step5_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

def get_cache_file(query_text: str) -> str:
    """Return the path for a cached response for a query."""
    query_hash = hashlib.md5(query_text.encode("utf-8")).hexdigest()
    return os.path.join(CACHE_DIR, f"{query_hash}.json")

def generate_citation_bound_answer_cached(query_text: str, top_k: int = 5):
    """Generate or load a citation-bound answer using cache."""
    cache_file = get_cache_file(query_text)
    
    # Return cached response if it exists
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            return json.load(f)
    
    # Otherwise, generate answer
    response = generate_citation_bound_answer(query_text, top_k=top_k)
    
    # Save to cache
    with open(cache_file, "w", encoding="utf-8") as f:
        json.dump(response, f, indent=2)
    
    return response

# -------------------------------
# Configure OpenAI API Key
# -------------------------------
openai.api_key = os.getenv("OPENAI_API_KEY")
if not openai.api_key:
    raise EnvironmentError("OPENAI_API_KEY environment variable not set")

# -------------------------------
# GPT-5 mini call
# -------------------------------
def llm_call(prompt: str) -> str:
    """
    Call GPT-5 mini using OpenAI >=1.0.0
    """
    response = openai.chat.completions.create(
        model="gpt-5-mini",
        messages=[
            {"role": "system", "content": "You are a compliance-aware AI. Answer strictly using provided source chunks."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content.strip()

# -------------------------------
# Citation-Bound Answer Generation
# -------------------------------
def generate_citation_bound_answer(query_text: str, top_k: int = 5):
    """
    Retrieve relevant chunks from CSSF, DORA, EBA and generate a citation-bound answer
    """
    regulators = {
        "CSSF": {"vector_store_key": "cssf", "authority": "CSSF", "jurisdiction": "LU"},
        "DORA": {"vector_store_key": "dora", "authority": "European Union", "jurisdiction": "EU"},
        "EBA": {"vector_store_key": "eba", "authority": "European Banking Authority", "jurisdiction": "EU"}
    }

    retrieved_chunks_all = []
    llm_input = ""
    similarity_scores = []

    # Multi-regulator retrieval
    for reg_name, reg_info in regulators.items():
        retrieval = retrieve(
            query_text=query_text,
            vector_store_key=reg_info["vector_store_key"],
            authority=reg_info["authority"],
            jurisdiction=reg_info["jurisdiction"],
            top_k=top_k
        )

        for chunk_info in retrieval["retrieved_chunks"]:
            metadata_list = vector_store[reg_info["vector_store_key"]]["metadata"]
            chunk_meta = next((c for c in metadata_list if c["chunk_id"] == chunk_info["chunk_id"]), None)
            if not chunk_meta:
                continue
            # Use regulator name from iteration, not missing field
            regulator_name = reg_name
            source_ref = chunk_meta.get("source_reference", chunk_info["source_reference"])
            llm_input += f"[{regulator_name} {source_ref}] {chunk_meta['text']}\n"
            retrieved_chunks_all.append({
                "chunk_id": chunk_meta["chunk_id"],
                "source_reference": source_ref,
                "source_regulation": regulator_name,
                "similarity_score": chunk_info["similarity_score"]
            })
            similarity_scores.append(chunk_info["similarity_score"])

    # Compute answer confidence
    answer_confidence = round(sum(similarity_scores)/len(similarity_scores), 4) if similarity_scores else 0.0

    # Construct strict citation-bound prompt
    prompt = f"""
            You are a compliance-aware AI. Answer the regulatory question strictly using the provided source chunks.
            Do NOT hallucinate. Cite sources inline in the format [REGULATION chunk_id / article / paragraph].
            If information is missing, respond: "Information not available in retrieved sources."

            Question: {query_text}

            Source Chunks:
            {llm_input}

            Answer:
            """

    # Generate answer using GPT-5 mini
    answer = llm_call(prompt)

    # Structured response
    response = {
        "query": query_text,
        "answer": answer,
        "answer_confidence": answer_confidence,
        "retrieved_chunks": retrieved_chunks_all,
        "retrieval_filters": {reg: {"authority": info["authority"], "jurisdiction": info["jurisdiction"]} for reg, info in regulators.items()},
        "timestamp": datetime.now().isoformat()
    }

    return response

# -------------------------------
# Example usage
# -------------------------------
if __name__ == "__main__":
    query_text = "Explain reporting obligations for EU financial entities under CSSF, DORA, and EBA."
    response = generate_citation_bound_answer(query_text)
    print(json.dumps(response, indent=2))
